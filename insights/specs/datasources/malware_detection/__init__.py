import os
import re
import time
import json
from sys import exit
import logging
from glob import glob
from datetime import datetime
from tempfile import NamedTemporaryFile, gettempdir

try:
    # python 2
    from urllib import quote as urlencode
except ImportError:
    # python 3
    from urllib.parse import quote as urlencode

from insights.client.connection import InsightsConnection
from insights.client.constants import InsightsConstants as constants
from insights.client.utilities import generate_machine_id, write_data_to_file, get_time
from insights.core.exceptions import CalledProcessError
from insights.util.subproc import call
from insights.specs.datasources.malware_detection import utils
from insights.specs.datasources.malware_detection import config
from insights.specs.datasources.malware_detection import rules as rules_utils

logger = logging.getLogger(__name__)


class MalwareDetectionClient:
    def __init__(
        self,
        insights_config,
        malware_detection_config,
        yara_binary_location,
        yara_version,
    ):
        # insights_config is used to access insights-client auth and network config when downloading rules
        self.insights_config = insights_config

        # set the malware-detection config file
        self.config = malware_detection_config

        # Set location of yara binary and the version of yara
        self.yara_binary = yara_binary_location
        self.yara_version = yara_version

        self.nice_value = self._validate_option("nice_value", 19)
        self.scan_timeout = self._validate_option("scan_timeout", 3600)
        self.cpu_thread_limit = self._validate_option("cpu_thread_limit", 2)
        self.string_match_limit = self._validate_option("string_match_limit", 10)
        # # Get/set the values of assorted integer config values - mainly options used with the yara command

        # new
        self.test_scan = self._get_config_option("test_scan", False)
        self.filesystem_scan_exclude_list = []
        self.processes_scan_exclude_list = []
        self.filesystem_scan_since_dict = {}
        self.processes_scan_since_dict = {}
        self.network_filesystem_mountpoints = []
        # new
        self.do_filesystem_scan = self._get_config_option("scan_filesystem", True)
        self.scan_fsobjects = []
        self.do_process_scan = self._get_config_option("scan_processes", False)
        self.scan_pids = []

        # If doing a test scan, then ignore the other scan_* options because test scan sets its own values for them
        # if not self._parse_test_scan_option():
        #     self._parse_scan_options()

        # Obtain the rules to be used by yara & the list of disabled rules, if any
        self.rules_files = []
        # new
        self.use_remote_rules = self._get_config_option("use_remote_rules", "True")
        self.remote_rules_location = self._get_config_option(
            "remote_rules_location", ""
        )
        # todo should this be False or ''
        self.rules_location = self._get_config_option("rules_location", "/etc/insights-client/signatures")

        # exclude rules directory from scans
        if self.rules_location:
            self.filesystem_scan_exclude_list.append(self.rules_location)

        self.disabled_rules = []

        # new setup connection
        self.conn = None

        if self.use_remote_rules or self.test_scan:
            # Check if insights-config is defined first because we need to access its auth and network config
            if not self.insights_config:
                logger.error("Couldn't access the insights-client configuration")
                exit(constants.sig_kill_bad)
            self.conn = InsightsConnection(self.insights_config)

        ca_cert = rules_utils.get_ca_certificate()
        self.cert_verify = rules_utils.get_cert_verify_value(
            self.insights_config
        )  # can be none
        self.custom_cert = self._get_config_option("ca_cert", ca_cert)  # can be none
        self.compiled_rules_flags = []
        self.non_compiled_files = []
        self.compiled_files = []

        # # Build the yara command, with its various command line options, that will be run
        # self._parse_rules_file()
        self.yara_cmd = []
        self.yara_compiled_cmds = []

        # host_scan is a dictionary into which all the scan matches are stored.  Its structure is like:
        # host_scan = {rule_name: [{source: ..., stringData: ..., stringIdentifier: ..., stringOffset: ...},
        #                          {source: ...}],
        #              rule_name: [{...}, {...}, {...}],
        #              ... }
        # host_scan_mutation is the host_scan dict converted to a GraphQL mutation query string
        self.host_scan = {}
        self.host_scan_mutation = ""

        # Check if we are adding extra metadata to each scan match
        self.add_metadata = self._get_config_option("add_metadata", False)

        self.matches = 0
        self.potential_matches = 0

    def parse_scan_options(self):
        if self.test_scan:
            return self._parse_test_scan_option()
        return self._parse_scan_options()

    def load_rules(self):
        rules_utils.remove_old_rules_files()

        if rules_utils.is_local_file(self.rules_location):
            self.rules_files = rules_utils.use_local_rules(self.rules_location)

        if self.use_remote_rules or self.test_scan:
            # Check if insights-config is defined first because we need to access its auth and network config
            if not self.insights_config:
                logger.error("Couldn't access the insights-client configuration")
                exit(constants.sig_kill_bad)
            if rules_utils.is_local_file(self.remote_rules_location):
                logger.error("REMOTE_RULES_LOCATION should be an url not local file")
                exit(constants.sig_kill_bad)
            self.remote_rules_location = rules_utils.set_remote_rules_location(
                self.insights_config, self.remote_rules_location, self.yara_version
            )
            self.remote_rules_location, authmethod = (
                rules_utils.validate_rules_location(
                    self.insights_config, self.remote_rules_location
                )
            )
            if authmethod == "CERT":
                self.insights_config.authmethod = "CERT"

            # If doing a test scan, replace signatures.yar (or any other file suffix) with test-rule.yar
            log_rule_contents, self.remote_rules_location = (
                rules_utils.handle_test_scan(self.test_scan, self.remote_rules_location)
            )

            logger.debug("Downloading rules from: %s", self.remote_rules_location)
            logger.debug("Using cert_verify value %s ...", self.cert_verify)

            response = rules_utils.download_rules(
                self.conn,
                self.insights_config,
                log_rule_contents,
                self.remote_rules_location,
                self.cert_verify,
                self.custom_cert,
            )

            # Can't abstract further as context manager closes deleting temp file
            self.temp_rules_file = NamedTemporaryFile(
                prefix="malware-detection_yara_rules.",
                mode="wb",
                delete=True,
                dir=config.RULE_DOWNLOAD_DIR,
            )
            self.temp_rules_file.write(response.content)
            self.temp_rules_file.flush()

            self.rules_files.append(self.temp_rules_file.name)

    def load_disabled_rules(self):
        """
        Download the list of disabled rules, if any.
        Doesn't apply if doing a test scan or if the rules are in a local file
        Uses network connection information set via the _get_rules method, eg conn, rules_location & cert_verify
        """
        disabled_rules = []
        if self.test_scan or not self.use_remote_rules:
            return disabled_rules

        graphql_url = self._set_url_path(self.remote_rules_location, "graphql")

        disabled_rules_query = (
            "query { rulesList(condition: {isDisabled: true}) { name } }"
        )
        for attempt in range(1, self.insights_config.retries + 1):
            try:
                response = self.conn.post(
                    graphql_url,
                    json={"query": disabled_rules_query},
                    headers={"Content-Type": "application/json"},
                    verify=self.cert_verify,
                    stream=True,
                )
                if response.status_code != 200:
                    raise Exception(
                        "%s %s: %s"
                        % (response.status_code, response.reason, response.text)
                    )

                # Retrieved response from graphql endpoint, parse out the list of disabled rules
                payload = response.json()
                if payload.get("data", {}).get("rulesList", []):
                    disabled_rules = list(
                        map(lambda x: x["name"], payload["data"]["rulesList"])
                    )
                break
            except Exception as e:
                if attempt < self.insights_config.retries:
                    logger.debug(
                        "Unable to get disabled rules list from %s: %s",
                        graphql_url,
                        str(e),
                    )
                    logger.debug("Trying again in %d seconds ...", attempt)
                    time.sleep(attempt)
                else:
                    logger.debug("Unable to get disabled rules list.  Skipping ...")

        logger.debug("Disabled rules: %s", disabled_rules)
        self.disabled_rules = sorted(map(lambda x: x.lower(), disabled_rules))

    def build_yara_commands(self):
        self._parse_rules_file()
        self.yara_cmd = self._build_yara_command()
        self.yara_compiled_cmds = self._build_compiled_yara_command()

    def run(self):
        def _process_scan_results():
            # If any scans were performed then get the results as a GraphQL mutation query
            # This mutation query is what is uploaded to the malware backend
            host_scan_mutation = self._create_host_scan_mutation()

            # Write a message to user informing them if there were matches or not and what to do next
            if self.matches == 0:
                if self.potential_matches == 0:
                    logger.info("No rule matches found.\n")
                else:
                    logger.info(
                        "Rule matches potentially found but problems encountered parsing them, so no match data to upload."
                    )
                    logger.info("Please contact support.\n")
            else:
                logger.info(
                    "Found %d rule match%s.",
                    self.matches,
                    "es" if self.matches > 1 else "",
                )
                if not self.test_scan:
                    logger.info(
                        "Please visit %s for more information\n", config.MALWARE_APP_URL
                    )

            # This is what is uploaded to the malware backend
            return (host_scan_mutation, self.host_scan)

        if not self.do_filesystem_scan and not self.do_process_scan:
            logger.error("No scans performed, no results to upload.")
            exit(constants.sig_kill_bad)

        results = []
        logger.debug("rules found: %s", self.rules_files)

        # Run at most two iterations for filesystem and processes. Concantenate the results.
        for idx, scan_op in enumerate([self.scan_filesystem, self.scan_processes]):
            if idx == 0:
                filesystem_scan_start = get_time()
            else:
                processes_scan_start = get_time()

            if self.yara_cmd:
                scan_results = scan_op()  # Perform scan operation
                if scan_results:
                    results = _process_scan_results()

            # Run scan for compiled rules
            for cmd in self.yara_compiled_cmds:
                self.yara_cmd = cmd
                scan_results = scan_op()  # Perform scan operation
                if scan_results:
                    results = _process_scan_results()

        # Write the scan start times to disk if scans were performed
        # (used by the 'filesystem_scan_since: last' and 'processes_scan_since: last' options)
        # Only write the scan time after scans have completed without error or interruption, and its not a test scan
        if not self.test_scan:
            if self.do_filesystem_scan:
                write_data_to_file(
                    filesystem_scan_start, config.LAST_FILESYSTEM_SCAN_FILE
                )
                os.chmod(config.LAST_FILESYSTEM_SCAN_FILE, 0o644)
            if self.do_process_scan:
                write_data_to_file(
                    processes_scan_start, config.LAST_PROCESSES_SCAN_FILE
                )
                os.chmod(config.LAST_PROCESSES_SCAN_FILE, 0o644)
        else:
            logger.info(
                "\nRed Hat Insights malware-detection app test scan complete.\n"
                "Test scan results are not recorded in the Insights UI (%s)\n"
                "To perform proper scans, please set test_scan: false in %s\n",
                config.MALWARE_APP_URL,
                config.MALWARE_CONFIG_FILE,
            )

        return results

    def _parse_scan_options(self):
        """
        Initialize the various scan flags and lists and run methods that may change/populate them
        """
        # self.do_filesystem_scan = self._get_config_option("scan_filesystem", True)
        # self.do_process_scan = self._get_config_option("scan_processes", False)

        if not (self.do_filesystem_scan or self.do_process_scan):
            logger.error(
                "Both scan_filesystem and scan_processes are disabled.  Nothing to scan."
            )
            exit(constants.sig_kill_bad)

        # Check if old options are still in use and inform the user of their replacements
        for replaced_scan_option in ("scan_only", "scan_exclude", "scan_since"):
            if self._get_config_option(replaced_scan_option):
                logger.error(
                    "The '{0}' option has been replaced with the 'filesystem_{0}' and 'processes_{0}' options in {1}".format(
                        replaced_scan_option, config.MALWARE_CONFIG_FILE
                    )
                )
                logger.error(
                    "Please remove the %s file and a new config file will be written with the new options",
                    config.MALWARE_CONFIG_FILE,
                )
                exit(constants.sig_kill_bad)

        # Try parsing the filesystem and processes scan_only options and exit under certain conditions
        parse_filesystem_scan_only = self._parse_filesystem_scan_only_option()
        parse_processes_scan_only = self._parse_processes_scan_only_option()
        if not (parse_filesystem_scan_only or parse_processes_scan_only):
            logger.error(
                "Nothing to scan with the filesystem_scan_only and processes_scan_only options"
            )
            exit(constants.sig_kill_bad)
        if not (parse_filesystem_scan_only or self.do_process_scan):
            logger.error(
                "Nothing to scan with filesystem_scan_only option and scan_processes is disabled"
            )
            exit(constants.sig_kill_bad)
        if not (self.do_filesystem_scan or parse_processes_scan_only):
            logger.error(
                "Nothing to scan with processes_scan_only option and scan_filesystem is disabled"
            )
            exit(constants.sig_kill_bad)

        # If we've made it here we are still doing scans, but disable scans if there were problems with scan_only
        if not parse_filesystem_scan_only:
            self.do_filesystem_scan = False
        if not parse_processes_scan_only:
            self.do_process_scan = False

        self._parse_filesystem_scan_exclude_option()
        self._parse_processes_scan_exclude_option()
        self._parse_filesystem_scan_since_option()
        self._parse_processes_scan_since_option()
        self._parse_exclude_network_filesystem_mountpoints_option()

    def _parse_test_scan_option(self):
        self.filesystem_scan_since_dict = {"timestamp": None}
        self.processes_scan_since_dict = {"timestamp": None}

        # For matching the test rule, scan the insights config file and the currently running process
        # Make sure the config file exists first though!
        if os.path.isfile(config.MALWARE_CONFIG_FILE):
            self.do_filesystem_scan = True
            self.scan_fsobjects = [config.MALWARE_CONFIG_FILE]
        else:
            self.do_filesystem_scan = False
            self.scan_fsobjects = []

        self.do_process_scan = True
        self.scan_pids = [str(os.getpid())]
        logger.info(
            "\nPerforming a test scan of %sthe current process (PID %s) "
            "to verify the malware-detection app is installed and scanning correctly ...\n",
            "%s and " % self.scan_fsobjects[0] if self.do_filesystem_scan else "",
            self.scan_pids[0],
        )

    def _parse_filesystem_scan_only_option(self):
        """
        Parse the filesystem_scan_only option, if specified, to get a list of files/dirs to scan
        If parsing was successful, then self.scan_fsobjects is populated and true is returned
        If parsing was not successful, self.scan_fsobjects remains empty and false is returned
        """
        filesystem_scan_only = self._get_config_option("filesystem_scan_only")
        if filesystem_scan_only:
            if not self.do_filesystem_scan:
                logger.error(
                    "Skipping filesystem_scan_only option because scan_filesystem is false"
                )
                return False
            # Process the filesystem_scan_only option as a list of files/dirs
            if not isinstance(filesystem_scan_only, list):
                filesystem_scan_only = [filesystem_scan_only]
            for item in filesystem_scan_only:
                # Remove extras slashes (/) in the file name and leading double slashes too (normpath doesn't)
                item = os.path.normpath(item).replace("//", "/")
                # Assume the item represents a filesystem item
                if not os.path.exists(item):
                    logger.info(
                        "Skipping missing filesystem_scan_only item: '%s'", item
                    )
                elif os.path.islink(item):
                    logger.info(
                        "Skipping symlink filesystem_scan_only item: '%s'.  Please use non-symlink items",
                        item,
                    )
                else:
                    self.scan_fsobjects.append(item)

            if self.scan_fsobjects:
                logger.info(
                    "Scan only the specified filesystem item%s: %s",
                    "s" if len(self.scan_fsobjects) > 1 else "",
                    self.scan_fsobjects,
                )
                return True
            else:
                logger.error(
                    "Unable to find the items specified for the filesystem_scan_only option.  Skipping ..."
                )
                return False
        return True

    def _parse_filesystem_scan_exclude_option(self):
        """
        Simple parse of the filesystem_scan_exclude option (if specified) to get a list of valid items to exclude
        """
        if not self.do_filesystem_scan:
            return

        filesystem_scan_exclude = self._get_config_option("filesystem_scan_exclude")
        if filesystem_scan_exclude:
            if not isinstance(filesystem_scan_exclude, list):
                # Convert filesystem_scan_exclude to a list if only a single non-list item was specified
                filesystem_scan_exclude = [filesystem_scan_exclude]
            for item in filesystem_scan_exclude:
                item = os.path.normpath(item).replace("//", "/")
                if not os.path.exists(item):
                    logger.info(
                        "Skipping missing filesystem_scan_exclude item: '%s'", item
                    )
                elif os.path.islink(item):
                    logger.info(
                        "Skipping symlink filesystem_scan_exclude item: '%s'.  Please use non-symlink items",
                        item,
                    )
                else:
                    self.filesystem_scan_exclude_list.append(item)
            if self.filesystem_scan_exclude_list:
                logger.info(
                    "Excluding specified filesystem item%s: %s",
                    "s" if len(self.filesystem_scan_exclude_list) > 1 else "",
                    self.filesystem_scan_exclude_list,
                )
            else:
                logger.info(
                    "Unable to find the items specified for the filesystem_scan_exclude option.  Not excluding any filesystem items"
                )

    @staticmethod
    def _parse_processes_scan_option(option_items):
        """
        'option_items' is the list of items provided for either the processes_scan_only or processes_scan_exclude options
        It is parsed as a list of items that may contain:
        - a single PID, eg 1
        - a range of PIDs, eg 10..100 or 10000..  or  ..500
        - a process_name, eg chrome

        A list of PIDs is returned representing all the PIDs that were matched from parsing the items
        """
        pids = []
        ps_output = call([["ps", "-eo", "pid=", "-o", "comm="]]).splitlines()
        proc_names = list(
            map(
                lambda x: (int(x[0]), str(x[1])),
                map(lambda x: tuple(x.split()), ps_output),
            )
        )
        proc_pids = list(map(lambda x: x[0], proc_names))
        if not isinstance(option_items, list):
            option_items = [str(option_items)]
        for item in option_items:
            if isinstance(item, float):
                # Handle floats so they don't cause exceptions
                item = str(item)
            if isinstance(item, int) or item.isdigit():
                # If it's digit, assume it represents a process ID
                if int(item) in proc_pids:
                    logger.debug("Found PID %s", item)
                    pids.append(str(item))
                else:
                    logger.info("Skipping missing PID: %s", item)
            elif ".." in item:
                # Assume the item represents a range of process IDs
                try:
                    start, end = item.split("..", 1)
                    start = 1 if not start else int(start.strip("."))
                    end = (
                        int(open("/proc/sys/kernel/pid_max").read())
                        if not end
                        else int(end.strip("."))
                    )
                    pid_matches = [
                        str(proc) for proc in proc_pids if start <= proc <= end
                    ]
                except Exception as err:
                    logger.error(
                        "Unable to parse '%s' in to a range of PIDs: %s", item, str(err)
                    )
                    continue
                logger.debug("Found PID(s) in range '%s': %s", item, pid_matches)
                if pid_matches:
                    pids.extend(pid_matches)
                else:
                    logger.info("No PIDs found in process range '%s'", item)
            else:
                # Assume the item is a string representing the name of one or multiple processes
                pid_matches = [str(proc[0]) for proc in proc_names if item in proc[1]]
                if pid_matches:
                    pids.extend(pid_matches)
                    logger.debug("Found PID(s) for string '%s': %s", item, pid_matches)
                else:
                    logger.info("No PID matches found for process name '%s'", item)

        return pids

    def _parse_processes_scan_only_option(self):
        """
        Parse the processes_scan_only option, if specified, to get a list of processes to scan
        """
        processes_scan_only = self._get_config_option("processes_scan_only")
        if processes_scan_only:
            if not self.do_process_scan:
                logger.error(
                    "Skipping processes_scan_only option because scan_processes is false"
                )
                return False

            pids = self._parse_processes_scan_option(processes_scan_only)
            if pids:
                self.scan_pids = sorted(set(pids), key=lambda pid: int(pid))
                logger.info(
                    "Scan only the specified process ID%s: %s",
                    "s" if len(self.scan_pids) > 1 else "",
                    self.scan_pids,
                )
                return True
            else:
                logger.error(
                    "Unable to find the items specified for the processes_scan_only option.  Skipping ..."
                )
                return False
        return True

    def _parse_processes_scan_exclude_option(self):
        """
        Simple parse of the processes_scan_exclude option (if specified) to get a list of processes to exclude
        """
        if not self.do_process_scan:
            return

        self.processes_scan_exclude_list = []
        processes_scan_exclude = self._get_config_option("processes_scan_exclude")
        if processes_scan_exclude:
            if not self.do_process_scan:
                logger.error(
                    "Skipping processes_scan_exclude option because scan_processes is false"
                )
                return

            pids = self._parse_processes_scan_option(processes_scan_exclude)
            if pids:
                self.processes_scan_exclude_list = sorted(
                    set(pids), key=lambda pid: int(pid)
                )
                logger.info(
                    "Excluding specified process ID%s: %s",
                    "s" if len(self.processes_scan_exclude_list) > 1 else "",
                    self.processes_scan_exclude_list,
                )
            else:
                logger.error(
                    "Unable to find the items specified for the processes_scan_exclude option.  Not excluding any processes."
                )

    def _parse_filesystem_scan_since_option(self):
        """
        filesystem_scan_since is specified as an integer representing the number of days ago to scan for modified files
        If the option was specified and valid, then get the corresponding unix timestamp for the specified
        number of days ago from now, which is used for comparing file modification times
        """
        if not self.do_filesystem_scan:
            return

        self.filesystem_scan_since_dict = {"timestamp": None, "datetime": None}
        filesystem_scan_since = self._get_config_option("filesystem_scan_since")
        if filesystem_scan_since is not None:
            timestamp = utils.get_scan_since_timestamp(
                "filesystem_scan_since", filesystem_scan_since
            )
            if timestamp:
                self.filesystem_scan_since_dict["timestamp"] = timestamp
                self.filesystem_scan_since_dict["datetime"] = datetime.fromtimestamp(
                    timestamp
                ).strftime("%Y-%m-%d %H:%M:%S")
                message = "Scan for files created/modified since %s%s"
                if isinstance(filesystem_scan_since, str):
                    submessage = "last successful scan on "
                else:
                    submessage = "%s day%s ago on " % (
                        filesystem_scan_since,
                        "s" if filesystem_scan_since > 1 else "",
                    )
                logger.info(
                    message, submessage, self.filesystem_scan_since_dict["datetime"]
                )

    def _parse_processes_scan_since_option(self):
        """
        processes_scan_since is specified as an integer representing the number of days ago to scan for new processes
        If the option was specified and valid, then get the corresponding unix timestamp for the specified
        number of days ago from now, which is used for comparing process start times
        """
        if not self.do_process_scan:
            return

        self.processes_scan_since_dict = {"timestamp": None, "datetime": None}
        processes_scan_since = self._get_config_option("processes_scan_since")
        if processes_scan_since is not None:
            timestamp = utils.get_scan_since_timestamp(
                "processes_scan_since", processes_scan_since
            )
            if timestamp:
                self.processes_scan_since_dict["timestamp"] = timestamp
                self.processes_scan_since_dict["datetime"] = datetime.fromtimestamp(
                    timestamp
                ).strftime("%Y-%m-%d %H:%M:%S")
                message = "Scan for processes started since %s%s"
                if isinstance(processes_scan_since, str):
                    submessage = "last successful scan on "
                else:
                    submessage = "%s day%s ago on " % (
                        processes_scan_since,
                        "s" if processes_scan_since > 1 else "",
                    )
                logger.info(
                    message, submessage, self.processes_scan_since_dict["datetime"]
                )

    def _parse_exclude_network_filesystem_mountpoints_option(self):
        """
        If exclude_network_filesystem_mountpoints is true, get a list of mountpoints of mounted network filesystems.
        The network_filesystem_types option has the list of network filesystems types to look for mountpoints for,
        eg NFS, CIFS, SMBFS, SSHFS, Ceph, GlusterFS, GFS.
        The list of network filesystem mountpoints will be added to the list of directories to exclude from scanning
        """
        if not self.do_filesystem_scan:
            return

        self.network_filesystem_mountpoints = []
        if not self._get_config_option("exclude_network_filesystem_mountpoints"):
            # We aren't excluding network filesystems, leave it as a blank list (ie nothing to exclude)
            return

        network_filesystem_types = self._get_config_option("network_filesystem_types")
        if not network_filesystem_types:
            logger.error("No value specified for 'network_filesystem_types' option")
            exit(constants.sig_kill_bad)

        if isinstance(network_filesystem_types, list):
            network_filesystem_types = ",".join(network_filesystem_types)
        cmd = ["findmnt", "-t", network_filesystem_types, "-n", "-o", "TARGET"]
        logger.debug("Command to find mounted network filesystems: %s", " ".join(cmd))
        try:
            output = call([cmd])
        except CalledProcessError as err:
            logger.error(
                "Unable to get network filesystem mountpoints: %s", err.output.strip()
            )
            exit(constants.sig_kill_bad)

        self.network_filesystem_mountpoints = (
            str(output).strip().split("\n") if output else []
        )
        if self.network_filesystem_mountpoints:
            logger.info(
                "Excluding network filesystem mountpoints: %s",
                self.network_filesystem_mountpoints,
            )
        else:
            logger.debug("No mounted network filesystems found")

    def _parse_rules_file(self):
        """
        Get all the switches for the yara command to be run, for example:
        - whether the rules file is compiled or not (-C)
        - the number of CPU threads to use (-p)
        - the nice command and its value to use (nice -n 'value')
        - scan timeouts (-a)
        Returns:
            - yara_cmd: the command for non-compiled rules files
            - yara_compiled_cmd: a list of commands, one per compiled rules file due to yara limitation
        """

        for rules_file in self.rules_files:
            output = call([["file", "-b", rules_file]])
            rule_type = output.strip().lower()
            if os.path.getsize(rules_file) == 0 or rule_type == "empty":
                logger.error("Rules file %s is empty", rules_file)
                exit(constants.sig_kill_bad)

            if rule_type.startswith("yara") or rule_type == "data":
                self.compiled_files.append(rules_file)
                self.compiled_rules_flags.append(
                    "-C"
                )  # Mark compiled rules for separate handling
            else:
                self.non_compiled_files.append(rules_file)
                self.compiled_rules_flags.append("")  # Non-compiled files

            logger.debug(
                "Rules file %s type: '%s', Compiled rules: %s",
                rules_file,
                rule_type,
                self.compiled_rules_flags[-1] == "-C",
            )

        # Quickly test each rules file to make sure it contains usable rules!
        for rules_file, compiled_flag in zip(
            self.rules_files, self.compiled_rules_flags
        ):
            cmd = list(
                filter(
                    None,
                    [
                        self.yara_binary,
                        "--fail-on-warnings",
                        "-p",
                        "1",
                        "-f",
                        compiled_flag,
                        rules_file,
                        "/dev/null",
                    ],
                )
            )
            try:
                call([cmd])
            except CalledProcessError as err:
                err_str = str(err.output.strip().decode())
                logger.error("Unable to use rules file %s: %s", rules_file, err_str)
                exit(constants.sig_kill_bad)

        # Limit the number of threads used by yara to limit the CPU load of the scans
        nproc = call("nproc").strip()
        if not nproc or int(nproc) <= 2:
            self.cpu_thread_limit = 1
        logger.debug("Using %s CPU thread(s) for scanning", self.cpu_thread_limit)

    def _build_yara_command(self):
        # Build yara_cmd for non-compiled rules
        yara_cmd = []
        if self.non_compiled_files:
            yara_cmd = list(
                filter(
                    None,
                    [
                        "nice",
                        "-n",
                        str(self.nice_value),
                        self.yara_binary,
                        "-s",
                        "-N",
                        "-a",
                        str(self.scan_timeout),
                        "-p",
                        str(self.cpu_thread_limit),
                        "-r",
                        "-f",
                    ]
                    + self.non_compiled_files,
                )
            )

        logger.debug("Yara command for non-compiled rules: %s", yara_cmd)

        return yara_cmd

    def _build_compiled_yara_command(self):
        # Build yara_compiled_cmd, one command per compiled file
        yara_compiled_cmds = []
        for compiled_file in self.compiled_files:
            cmd = list(
                filter(
                    None,
                    [
                        "nice",
                        "-n",
                        str(self.nice_value),
                        self.yara_binary,
                        "-s",
                        "-N",
                        "-a",
                        str(self.scan_timeout),
                        "-p",
                        str(self.cpu_thread_limit),
                        "-r",
                        "-f",
                        "-C",
                        compiled_file,
                    ],
                )
            )
            yara_compiled_cmds.append(cmd)

        logger.debug("Yara commands for compiled rules: %s", yara_compiled_cmds)

        return yara_compiled_cmds

    def scan_filesystem(self):
        """
        Process the filesystem items to scan
        If self.scan_fsobjects is set, then just scan its items, less any items in the exclude list
        scan_dict will contain all the toplevel directories to scan, and any particular files/subdirectories to scan
        """
        logger.debug("-------filesystem scan with %s", self.yara_cmd)
        if not self.do_filesystem_scan:
            return False

        # Exclude the rules files and insights-client log files, unless they are things we specifically want to scan
        # Get a list of potential rules files locations, e.g., /tmp, /var/tmp, /usr/tmp, and gettempdir()
        # For example, customers may have /tmp linked to /var/tmp, so both must be checked for excluding the downloaded rules
        # Iterate over all rule files in the list `self.rules_files`
        potential_tmp_dirs = set([gettempdir(), "/tmp", "/var/tmp", "/usr/tmp"])

        # Loop over each rules file to ensure they are excluded from the scan
        for rule_file in self.rules_files:
            rules_file_name = os.path.basename(rule_file)

            # Generate potential locations where the rule files might be stored temporarily
            potential_rules_files = set(
                list(
                    map(lambda d: os.path.join(d, rules_file_name), potential_tmp_dirs)
                )
                + [rule_file]
            )

            # Check if these files exist and add them to the exclusion list if not already included in scan_fsobjects
            rules_files = list(
                filter(lambda f: os.path.isfile(f), potential_rules_files)
            )
            for rules_file in rules_files:
                if rules_file not in self.scan_fsobjects:
                    self.filesystem_scan_exclude_list.append(rules_file)
                    logger.debug("Excluding rules file: %s", rules_file)

        # Exclude insights-client log files
        insights_log_files = glob(constants.default_log_file + "*")
        self.filesystem_scan_exclude_list.extend(
            list(set(insights_log_files) - set(self.scan_fsobjects))
        )

        # Process the inclusion and exclusion lists
        scan_dict = utils.process_include_exclude_items(
            include_items=self.scan_fsobjects,
            exclude_items=self.filesystem_scan_exclude_list,
            exclude_mountpoints=self.network_filesystem_mountpoints,
        )
        if not scan_dict:
            self.do_filesystem_scan = False
            return False

        logger.debug(
            "Filesystem objects to be scanned in: %s", sorted(scan_dict.keys())
        )

        logger.info("Starting filesystem scan ...")
        fs_scan_start = time.time()

        for toplevel_dir in sorted(scan_dict):
            # Make a copy of the self.yara_cmd list and add to it the thing to scan
            cmd = self.yara_cmd[:]
            dir_scan_start = time.time()

            specified_log_txt = (
                "specified " if "include" in scan_dict[toplevel_dir] else ""
            )
            if self.filesystem_scan_since_dict["timestamp"]:
                logger.info(
                    "Scanning %sfiles in %s modified since %s ...",
                    specified_log_txt,
                    toplevel_dir,
                    self.filesystem_scan_since_dict["datetime"],
                )
                # Find the recently modified files in the given top level directory
                scan_list_file = NamedTemporaryFile(
                    prefix="%s_scan_list." % os.path.basename(toplevel_dir),
                    mode="w",
                    delete=True,
                )
                if "include" in scan_dict[toplevel_dir]:
                    utils.find_modified_include_items(
                        scan_dict[toplevel_dir]["include"],
                        self.filesystem_scan_since_dict["timestamp"],
                        scan_list_file,
                    )
                else:
                    utils.find_modified_in_directory(
                        toplevel_dir,
                        self.filesystem_scan_since_dict["timestamp"],
                        scan_list_file,
                    )

                scan_list_file.flush()
                cmd.extend(["--scan-list", scan_list_file.name])
            else:
                logger.info(
                    "Scanning %sfiles in %s ...", specified_log_txt, toplevel_dir
                )
                if "include" in scan_dict[toplevel_dir]:
                    scan_list_file = NamedTemporaryFile(
                        prefix="%s_scan_list." % os.path.basename(toplevel_dir),
                        mode="w",
                        delete=True,
                    )
                    scan_list_file.write("\n".join(scan_dict[toplevel_dir]["include"]))
                    scan_list_file.flush()
                    cmd.extend(["--scan-list", scan_list_file.name])
                else:
                    cmd.append(toplevel_dir)

            logger.debug("Yara command: %s", cmd)
            try:
                output = call([cmd]).strip()
            except CalledProcessError as cpe:
                logger.debug("Unable to scan %s: %s", toplevel_dir, cpe.output.strip())
                continue

            try:
                self.parse_scan_output(output.strip())
            except Exception as e:
                self.potential_matches += 1
                logger.exception(
                    "Rule match(es) potentially found in %s but problems encountered parsing the results: %s.  Skipping ...",
                    toplevel_dir,
                    str(e),
                )

            dir_scan_end = time.time()
            logger.info(
                "Scan time for %s: %d seconds",
                toplevel_dir,
                (dir_scan_end - dir_scan_start),
            )
            if dir_scan_end - dir_scan_start >= self.scan_timeout - 2:
                logger.warning(
                    "Scan of %s timed-out after %d seconds and may not have been fully scanned.  "
                    "Consider increasing the scan_timeout value in %s",
                    toplevel_dir,
                    self.scan_timeout,
                    config.MALWARE_CONFIG_FILE,
                )

        fs_scan_end = time.time()
        logger.info(
            "Filesystem scan time: %s",
            time.strftime("%H:%M:%S", time.gmtime(fs_scan_end - fs_scan_start)),
        )
        return True

    def scan_processes(self):
        if not self.do_process_scan:
            return False

        # Get a list of all PIDs to scan if none were specified with scan only option
        if not self.scan_pids:
            self.scan_pids = [entry for entry in os.listdir("/proc") if entry.isdigit()]

        # Add this currently running process' PID to the list of processes to exclude (unless its a test_scan)
        # Then remove the excluded processes from the list of PIDs to scan
        if not self.test_scan:
            self.processes_scan_exclude_list.append(
                str(os.getpid())
            )  # make sure to exclude our script's pid
        self.scan_pids = sorted(
            list(set(self.scan_pids) - set(self.processes_scan_exclude_list)),
            key=lambda pid: int(pid),
        )

        if not self.scan_pids:
            logger.error(
                "No processes to scan because the specified exclude items cancel them out"
            )
            self.do_process_scan = False
            return False

        # If process_scan_since is specified, get a list of processes started since the specified date
        if (
            hasattr(self, "processes_scan_since_dict")
            and self.processes_scan_since_dict["timestamp"]
        ):
            # First get a list of all running processes and their start times
            ps_output = call([["ps", "-eo", "pid=", "-o", "lstart="]]).splitlines()
            all_proc_starts = list(
                map(
                    lambda x: (str(x[0]), str(x[1])),
                    map(lambda x: tuple(x.strip().split(" ", 1)), ps_output),
                )
            )
            scan_since_pids = []
            # Loop through all processes and if the process start time <= the specified scan_since time then
            # make note of the process
            for proc in all_proc_starts:
                proc_start_secs = float(
                    datetime.strptime(proc[1], "%a %b %d %H:%M:%S %Y").strftime("%s")
                )
                if proc_start_secs >= self.processes_scan_since_dict["timestamp"]:
                    scan_since_pids.append(proc[0])
            # Finally, do a set intersection of the current list of scan_pids and the list of processes started
            # since processes_scan_since.  The resulting list is the list of processes to scan.
            self.scan_pids = sorted(
                list(set(self.scan_pids) & set(scan_since_pids)),
                key=lambda pid: int(pid),
            )

            if not self.scan_pids:
                logger.error(
                    "No processes to scan because none were started since %s",
                    self.processes_scan_since_dict["datetime"],
                )
                self.do_process_scan = False
                return False

        logger.info("Starting processes scan ...")
        pids_scan_start = time.time()

        for scan_pid in self.scan_pids:
            pid_scan_start = time.time()
            logger.info("Scanning process %s ...", scan_pid)
            cmd = self.yara_cmd + [str(scan_pid)]
            logger.debug("Yara command: %s", cmd)
            try:
                output = call([cmd]).strip()
            except CalledProcessError as cpe:
                logger.debug(
                    "Unable to scan process %s: %s", scan_pid, cpe.output.strip()
                )
                continue

            try:
                self.parse_scan_output(output)
            except Exception as e:
                self.potential_matches += 1
                logger.exception(
                    "Rule match(es) potentially found in process %s but problems encountered parsing the results: %s.  Skipping ...",
                    scan_pid,
                    str(e),
                )

            pid_scan_end = time.time()
            logger.info(
                "Scan time for process %s: %d seconds",
                scan_pid,
                (pid_scan_end - pid_scan_start),
            )
            if pid_scan_end - pid_scan_start >= self.scan_timeout - 2:
                logger.warning(
                    "Scan of process %s timed-out after %d seconds and may not have been fully scanned.  "
                    "Consider increasing the scan_timeout value in %s",
                    scan_pid,
                    self.scan_timeout,
                    config.MALWARE_CONFIG_FILE,
                )

        pids_scan_end = time.time()
        logger.info(
            "Processes scan time: %s",
            time.strftime("%H:%M:%S", time.gmtime(pids_scan_end - pids_scan_start)),
        )
        return True

    def parse_scan_output(self, output):
        if not output:
            return

        # Each 'set' of output lines consists of 1 line containing the rule and file/pid (aka source) it matches
        # Followed by one or more related lines of matching string data from that source, eg
        # ...
        # rule_name source                            + Set of 3 related lines
        # 0x_offset:string_identifier:string_data     |
        # 0x_offset:string_identifier:string_data     +
        # rule_name source                            + Set of 2 related lines
        # 0x_offset:string_identifier:string_data     +
        # ...

        def skip_string_data_lines(string_data_lines):
            # Skip the 0x... lines containing string match data
            while string_data_lines and string_data_lines[0].startswith("0x"):
                string_data_lines.pop(0)

        output_lines = output.split("\n")
        while output_lines:
            if "error scanning " in output_lines[0]:
                logger.debug("Skipping yara output line: %s", output_lines[0])
                output_lines.pop(0)  # Skip the error scanning line
                # Skip any string match lines after the error scanning line
                skip_string_data_lines(output_lines)
                continue
            # Get the rule_name and source from the first line in the set
            try:
                rule_name, source = output_lines[0].rstrip().split(" ", 1)
            except ValueError as err:
                # Hopefully shouldn't happen but log it and continue processing
                logger.debug(
                    "Error parsing rule match '%s': %s", output_lines[0], str(err)
                )
                output_lines.pop(0)  # Skip the erroneous line
                # Skip any string match lines afterwards until we get to the next rule match line
                skip_string_data_lines(output_lines)
                continue

            # All good so far, skip over the line containing the rule name and matching source file/pid
            output_lines.pop(0)

            # Check if the rule name contains a ':' or doesn't start with a char/string
            # It shouldn't and its likely to be due to a malformed string_offset line
            # Skip any further scan matches until the next rule match
            if ":" in rule_name or not re.match("^[a-zA-Z]+", rule_name):
                skip_string_data_lines(output_lines)
                continue

            source_type = "process" if source.isdigit() else "file"

            # If the rule is disabled, then skip over its scan matches until the next rule match
            if rule_name.lower() in self.disabled_rules:
                logger.debug(
                    "Skipping matches for disabled rule %s in %s %s",
                    rule_name,
                    source_type,
                    source,
                )
                skip_string_data_lines(output_lines)
                continue

            # Parse the string match data for the remaining lines in the set
            rule_match = {"rule_name": rule_name, "matches": []}
            string_matches = 0
            while output_lines and output_lines[0].startswith("0x"):
                if string_matches < self.string_match_limit:
                    try:
                        string_offset, string_identifier, string_data = output_lines[
                            0
                        ].split(":", 2)
                        string_offset = int(string_offset, 0)
                    except ValueError as err:
                        logger.debug(
                            "Error parsing string match '%s': %s", output_lines[0], err
                        )
                        output_lines.pop(0)
                        continue
                    rule_match_dict = {
                        "source": source,
                        "string_data": string_data.strip(),
                        "string_identifier": string_identifier,
                        "string_offset": string_offset,
                        "metadata": {"source_type": source_type},
                    }
                    rule_match["matches"].extend([rule_match_dict])
                output_lines.pop(0)
                string_matches += 1

            # If string_match_limit is 0 or there was no string data, there will be no rule_matches,
            # but still record the file/pid source that was matched
            if not rule_match["matches"]:
                rule_match_dict = {
                    "source": source,
                    "string_data": "",
                    "string_identifier": "",
                    "string_offset": -1,
                    "metadata": {"source_type": source_type},
                }
                rule_match["matches"] = [rule_match_dict]

            if self.add_metadata:
                try:
                    # Add extra data to each rule match, beyond what yara provides
                    # Eg, for files: line numbers & context, checksums; for processes: process name
                    # TODO: find more pythonic ways of doing this stuff instead of using system commands
                    metadata_func = (
                        self._add_file_metadata
                        if source_type == "file"
                        else self._add_process_metadata
                    )
                    metadata_func(rule_match["matches"])
                except Exception as e:
                    logger.error(
                        "Error adding metadata to rule match %s in %s %s: %s.  Skipping ...",
                        rule_name,
                        source_type,
                        source,
                        str(e),
                    )

            self.matches += 1
            logger.info("Matched rule %s in %s %s", rule_name, source_type, source)
            logger.debug(rule_match)
            if self.host_scan.get(rule_match["rule_name"]):
                self.host_scan[rule_match["rule_name"]].extend(rule_match["matches"])
            else:
                self.host_scan[rule_match["rule_name"]] = rule_match["matches"]

    def _add_process_metadata(self, rule_matches):
        """
        Add extra data to the process scan matches beyond what is provided by yara, eg process name
        """
        # All passed in rule_matches will have the same source PID
        # Check the process still exists before obtaining the metadata about it
        source = rule_matches[0]["source"]
        if not os.path.exists("/proc/%s" % source):
            return

        # Get name of process from ps command
        # -h: no output header, -q: only the specified process, -o args: just the process name and args
        try:
            process_name = call([["ps", "-hq", source, "-o", "args"]]).strip()
        except CalledProcessError:
            process_name = "unknown"

        for rule_match in rule_matches:
            rule_match["metadata"].update({"process_name": process_name})

    def _add_file_metadata(self, rule_matches):
        """
        Add extra data to the file scan matches beyond what is provided by yara
        - eg matching line numbers, line context, file checksum
        - Use grep to get the line numbers & sed to get the line
        """

        def get_line_from_file(file_name, line_number):
            # Extract the line at line_number from file_name
            line_length_limit = 120
            try:
                line = call([["sed", "%dq;d" % line_number, file_name]]).strip()
            except CalledProcessError:
                line = ""
            # Limit line length if necessary and urlencode it to minimize problems with GraphQL when uploading
            return urlencode(
                line
                if len(line) < line_length_limit
                else line[:line_length_limit] + "..."
            )

        # All passed in rule_matches will have the same source file
        # Check the file still exists before obtaining the metadata about it
        source = rule_matches[0]["source"]
        if not os.path.exists(source):
            return

        # Get the file type, mime type and md5sum hash of the source file
        try:
            file_type = call([["file", "-b", source]]).strip()
        except Exception:
            file_type = ""
        try:
            mime_type = call([["file", "-bi", source]]).strip()
        except Exception:
            mime_type = ""
        try:
            md5sum = call([["md5sum", source]]).strip().split()[0]
        except Exception:
            md5sum = ""

        grep_string_data_match_list = []
        if mime_type and "charset=binary" not in mime_type:
            # Get the line numbers for each of yara's string_data matches in the source file, but not for binary files
            # Build a grep command that searches for each of the string_data patterns in the source file
            # For each string_data pattern that grep finds, the grep output will have the form...
            # line_number:offset_from_0:string_data_pattern

            # Get the set of patterns to grep for, eg ['pattern1', 'pattern2', etc], ie remove duplicate patterns
            grep_string_data_pattern_set = set(
                [match["string_data"] for match in rule_matches]
            )
            if grep_string_data_pattern_set:
                # Build an option list for grep, eg ['-e', 'pattern1', '-e', 'pattern2', ... etc]
                # zip creates a list of tuples, eg [('-e', 'pattern'), ('-e', 'pattern2'), ...], then flatten the list
                grep_string_data_patterns = [
                    item
                    for tup in list(
                        zip(
                            ["-e"] * len(grep_string_data_pattern_set),
                            grep_string_data_pattern_set,
                        )
                    )
                    for item in tup
                ]
                # Create the grep command to execute.  -F means don't interpret regex special chars in the patterns
                grep_command = (
                    ["/bin/grep", "-Fbon"] + grep_string_data_patterns + [source]
                )
                logger.debug("grep command: %s", grep_command)
                try:
                    grep_output = call([grep_command])
                except CalledProcessError:
                    grep_output = ""

                # Now turn the grep output into a list of tuples for easier searching a little later, ie
                # [(line_number, offset_from_0, string_data_pattern), (...), ]
                if grep_output and not grep_output.lower().startswith("binary"):
                    grep_string_data_match_list = list(
                        map(
                            lambda grep_output_line: tuple(
                                grep_output_line.split(":", 3)
                            ),
                            grep_output.splitlines(),
                        )
                    )

        for rule_match in rule_matches:
            metadata = rule_match["metadata"]
            metadata.update(
                {"file_type": file_type, "mime_type": mime_type, "md5sum": md5sum}
            )
            if grep_string_data_match_list:
                # Now, for each offset_from_0 in the grep output, we want to match it with the corresponding
                # string_offset value from the yara output so we can get the line number for that string_data match
                # And while we are here, get the line from the source file at that line number
                line_number = None
                for grep_list_item in grep_string_data_match_list:
                    if int(grep_list_item[1]) == rule_match["string_offset"]:
                        line_number = int(grep_list_item[0])
                        break
                if line_number:
                    metadata.update(
                        {
                            "line_number": line_number,
                            "line": get_line_from_file(source, line_number),
                        }
                    )

    def _create_host_scan_mutation(self):
        # Build the mutation text
        mutation_header = (
            """
            mutation HostScan {
              recordHostScan(
                input: {
                  scannedhost: {
                    insightsId: "%s"
                    rulesScanned: ["""
            % generate_machine_id()
        )

        mutation_footer = """
                ]
              }
            }
          ) {
            success
          }
        }"""

        mutation = mutation_header
        for rule_name in self.host_scan.keys():
            rule_scan = (
                """{
                    ruleName: "%s"
                    stringsMatched: ["""
                % rule_name
            )
            for match in self.host_scan[rule_name]:
                rule_scan += """{
                    source: "%s"
                    stringData: %s
                    stringIdentifier: %s
                    stringOffset: "%s"
                    metadata: "%s"
                }, """ % (
                    match["source"],
                    json.dumps(match["string_data"]),
                    json.dumps(match["string_identifier"]),
                    match["string_offset"],
                    json.dumps(match["metadata"]).replace('"', '\\"'),
                )
            rule_scan += "]}, "
            mutation += rule_scan

        mutation += mutation_footer
        return mutation

    def _get_config_option(self, option, default_value=None):
        """
        Get the value of a config option or if it doesn't exist or is None, the default_value
        """
        value = os.getenv(option.upper())
        if value is not None:
            return config.parse_env_var(option.upper(), value)
        value = self.config.get(option)
        return value if value is not None else default_value

    @staticmethod
    def _set_url_path(url, path="graphql"):
        """
        Change the last item in the 'url' to 'path'
        For example if url=http://localhost:3000/api/malware/signatures.yar and path=test-rule.yar
        The returned url would be http://localhost:3000/api/malware/test-rule.yar
        """
        if url.endswith(path):
            return url
        # Unconventional perhaps, but this seems to work ok with both URL and file paths
        return os.path.join(os.path.dirname(url), path)

    def _validate_option(self, option, value):
        try:
            return int(self._get_config_option(option, value))
        except Exception as e:
            logger.error("Problem setting configuration option %s: %s", option, str(e))
            exit(constants.sig_kill_bad)
