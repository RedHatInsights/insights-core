import os
import logging
import json
import re

# Python 2/3 compatibility
try:
    # Python 2
    from urllib import quote as urlencode
except ImportError:
    # Python 3
    from urllib.parse import quote as urlencode

from insights.client.utilities import generate_machine_id
from insights.util.subproc import call
from insights.core.exceptions import CalledProcessError

logger = logging.getLogger(__name__)


class ScanResultParser:
    """
    Parses YARA scan output into structured results.

    Handles parsing of YARA output format, metadata enrichment,
    and conversion to GraphQL mutation format.
    """

    def __init__(self, config):
        """
        Initialize scan result parser.

        Args:
            get_config_option_func: Function to get config option values
            validate_option_func: Function to validate config option values
        """
        # Check if we are adding extra metadata to each scan match
        self.add_metadata = config.add_metadata
        self.string_match_limit = config.string_match_limit

        # Scan results storage
        self.host_scan = {}
        self.host_scan_mutation = ""
        self.matches = 0
        self.potential_matches = 0

    def create_host_scan_mutation(self):
        # Build the mutation text
        mutation_header = (
            """
            mutation HostScan {
              recordHostScan(
                input: {
                  scannedhost: {
                    insightsId: "%s"
                    rulesScanned: ["""
            % generate_machine_id()
        )

        mutation_footer = """
                ]
              }
            }
          ) {
            success
          }
        }"""

        mutation = mutation_header
        for rule_name in self.host_scan.keys():
            rule_scan = (
                """{
                    ruleName: "%s"
                    stringsMatched: ["""
                % rule_name
            )
            for match in self.host_scan[rule_name]:
                rule_scan += """{
                    source: "%s"
                    stringData: %s
                    stringIdentifier: %s
                    stringOffset: "%s"
                    metadata: "%s"
                }, """ % (
                    match["source"],
                    json.dumps(match["string_data"]),
                    json.dumps(match["string_identifier"]),
                    match["string_offset"],
                    json.dumps(match["metadata"]).replace('"', '\\"'),
                )
            rule_scan += "]}, "
            mutation += rule_scan

        mutation += mutation_footer
        return mutation

    def _add_process_metadata(self, rule_matches):
        """
        Add extra data to the process scan matches beyond what is provided by yara, eg process name
        """
        # All passed in rule_matches will have the same source PID
        # Check the process still exists before obtaining the metadata about it
        source = rule_matches[0]["source"]
        if not os.path.exists("/proc/%s" % source):
            return

        # Get name of process from ps command
        # -h: no output header, -q: only the specified process, -o args: just the process name and args
        try:
            process_name = call([["ps", "-hq", source, "-o", "args"]]).strip()
        except CalledProcessError:
            process_name = "unknown"

        for rule_match in rule_matches:
            rule_match["metadata"].update({"process_name": process_name})

    def _add_file_metadata(self, rule_matches):
        """
        Add extra data to the file scan matches beyond what is provided by yara
        - eg matching line numbers, line context, file checksum
        - Use grep to get the line numbers & sed to get the line
        """

        def get_line_from_file(file_name, line_number):
            # Extract the line at line_number from file_name
            line_length_limit = 120
            try:
                line = call([["sed", "%dq;d" % line_number, file_name]]).strip()
            except CalledProcessError:
                line = ""
            # Limit line length if necessary and urlencode it to minimize problems with GraphQL when uploading
            return urlencode(
                line
                if len(line) < line_length_limit
                else line[:line_length_limit] + "..."
            )

        # All passed in rule_matches will have the same source file
        # Check the file still exists before obtaining the metadata about it
        source = rule_matches[0]["source"]
        if not os.path.exists(source):
            return

        # Get the file type, mime type and md5sum hash of the source file
        try:
            file_type = call([["file", "-b", source]]).strip()
        except Exception:
            file_type = ""
        try:
            mime_type = call([["file", "-bi", source]]).strip()
        except Exception:
            mime_type = ""
        try:
            md5sum = call([["md5sum", source]]).strip().split()[0]
        except Exception:
            md5sum = ""

        grep_string_data_match_list = []
        if mime_type and "charset=binary" not in mime_type:
            # Get the line numbers for each of yara's string_data matches in the source file, but not for binary files
            # Build a grep command that searches for each of the string_data patterns in the source file
            # For each string_data pattern that grep finds, the grep output will have the form...
            # line_number:offset_from_0:string_data_pattern

            # Get the set of patterns to grep for, eg ['pattern1', 'pattern2', etc], ie remove duplicate patterns
            grep_string_data_pattern_set = set(
                [match["string_data"] for match in rule_matches]
            )
            if grep_string_data_pattern_set:
                # Build an option list for grep, eg ['-e', 'pattern1', '-e', 'pattern2', ... etc]
                # zip creates a list of tuples, eg [('-e', 'pattern'), ('-e', 'pattern2'), ...], then flatten the list
                grep_string_data_patterns = [
                    item
                    for tup in list(
                        zip(
                            ["-e"] * len(grep_string_data_pattern_set),
                            grep_string_data_pattern_set,
                        )
                    )
                    for item in tup
                ]
                # Create the grep command to execute.  -F means don't interpret regex special chars in the patterns
                grep_command = (
                    ["/bin/grep", "-Fbon"] + grep_string_data_patterns + [source]
                )
                logger.debug("grep command: %s", grep_command)
                try:
                    grep_output = call([grep_command])
                except CalledProcessError:
                    grep_output = ""

                # Now turn the grep output into a list of tuples for easier searching a little later, ie
                # [(line_number, offset_from_0, string_data_pattern), (...), ]
                if grep_output and not grep_output.lower().startswith("binary"):
                    grep_string_data_match_list = list(
                        map(
                            lambda grep_output_line: tuple(
                                grep_output_line.split(":", 3)
                            ),
                            grep_output.splitlines(),
                        )
                    )

        for rule_match in rule_matches:
            metadata = rule_match["metadata"]
            metadata.update(
                {"file_type": file_type, "mime_type": mime_type, "md5sum": md5sum}
            )
            if grep_string_data_match_list:
                # Now, for each offset_from_0 in the grep output, we want to match it with the corresponding
                # string_offset value from the yara output so we can get the line number for that string_data match
                # And while we are here, get the line from the source file at that line number
                line_number = None
                for grep_list_item in grep_string_data_match_list:
                    if int(grep_list_item[1]) == rule_match["string_offset"]:
                        line_number = int(grep_list_item[0])
                        break
                if line_number:
                    metadata.update(
                        {
                            "line_number": line_number,
                            "line": get_line_from_file(source, line_number),
                        }
                    )

    def parse_scan_output(self, output, disabled_rules):
        if not output:
            return

        # Each 'set' of output lines consists of 1 line containing the rule and file/pid (aka source) it matches
        # Followed by one or more related lines of matching string data from that source, eg
        # ...
        # rule_name source                            + Set of 3 related lines
        # 0x_offset:string_identifier:string_data     |
        # 0x_offset:string_identifier:string_data     +
        # rule_name source                            + Set of 2 related lines
        # 0x_offset:string_identifier:string_data     +
        # ...

        def skip_string_data_lines(string_data_lines):
            # Skip the 0x... lines containing string match data
            while string_data_lines and string_data_lines[0].startswith("0x"):
                string_data_lines.pop(0)

        output_lines = output.split("\n")
        while output_lines:
            if "error scanning " in output_lines[0]:
                logger.debug("Skipping yara output line: %s", output_lines[0])
                output_lines.pop(0)  # Skip the error scanning line
                # Skip any string match lines after the error scanning line
                skip_string_data_lines(output_lines)
                continue
            # Get the rule_name and source from the first line in the set
            try:
                rule_name, source = output_lines[0].rstrip().split(" ", 1)
            except ValueError as err:
                # Hopefully shouldn't happen but log it and continue processing
                logger.debug(
                    "Error parsing rule match '%s': %s", output_lines[0], str(err)
                )
                output_lines.pop(0)  # Skip the erroneous line
                # Skip any string match lines afterwards until we get to the next rule match line
                skip_string_data_lines(output_lines)
                continue

            # All good so far, skip over the line containing the rule name and matching source file/pid
            output_lines.pop(0)

            # Check if the rule name contains a ':' or doesn't start with a char/string
            # It shouldn't and its likely to be due to a malformed string_offset line
            # Skip any further scan matches until the next rule match
            if ":" in rule_name or not re.match("^[a-zA-Z]+", rule_name):
                skip_string_data_lines(output_lines)
                continue

            source_type = "process" if source.isdigit() else "file"

            # If the rule is disabled, then skip over its scan matches until the next rule match
            if rule_name.lower() in disabled_rules:
                logger.debug(
                    "Skipping matches for disabled rule %s in %s %s",
                    rule_name,
                    source_type,
                    source,
                )
                skip_string_data_lines(output_lines)
                continue

            # Parse the string match data for the remaining lines in the set
            rule_match = {"rule_name": rule_name, "matches": []}
            string_matches = 0
            while output_lines and output_lines[0].startswith("0x"):
                if string_matches < self.string_match_limit:
                    try:
                        string_offset, string_identifier, string_data = output_lines[
                            0
                        ].split(":", 2)
                        string_offset = int(string_offset, 0)
                    except ValueError as err:
                        logger.debug(
                            "Error parsing string match '%s': %s", output_lines[0], err
                        )
                        output_lines.pop(0)
                        continue
                    rule_match_dict = {
                        "source": source,
                        "string_data": string_data.strip(),
                        "string_identifier": string_identifier,
                        "string_offset": string_offset,
                        "metadata": {"source_type": source_type},
                    }
                    rule_match["matches"].extend([rule_match_dict])
                output_lines.pop(0)
                string_matches += 1

            # If string_match_limit is 0 or there was no string data, there will be no rule_matches,
            # but still record the file/pid source that was matched
            if not rule_match["matches"]:
                rule_match_dict = {
                    "source": source,
                    "string_data": "",
                    "string_identifier": "",
                    "string_offset": -1,
                    "metadata": {"source_type": source_type},
                }
                rule_match["matches"] = [rule_match_dict]

            if self.add_metadata:
                try:
                    # Add extra data to each rule match, beyond what yara provides
                    # Eg, for files: line numbers & context, checksums; for processes: process name
                    # TODO: find more pythonic ways of doing this stuff instead of using system commands
                    metadata_func = (
                        self._add_file_metadata
                        if source_type == "file"
                        else self._add_process_metadata
                    )
                    metadata_func(rule_match["matches"])
                except Exception as e:
                    logger.error(
                        "Error adding metadata to rule match %s in %s %s: %s.  Skipping ...",
                        rule_name,
                        source_type,
                        source,
                        str(e),
                    )

            self.matches += 1
            logger.info("Matched rule %s in %s %s", rule_name, source_type, source)
            logger.debug(rule_match)
            if self.host_scan.get(rule_match["rule_name"]):
                self.host_scan[rule_match["rule_name"]].extend(rule_match["matches"])
            else:
                self.host_scan[rule_match["rule_name"]] = rule_match["matches"]

    def process_scan_results(self, config):
        # If any scans were performed then get the results as a GraphQL mutation query
        # This mutation query is what is uploaded to the malware backend
        self.host_scan_mutation = self.create_host_scan_mutation()

        # Write a message to user informing them if there were matches or not and what to do next
        if self.matches == 0:
            if self.potential_matches == 0:
                logger.info("No rule matches found.\n")
            else:
                logger.info(
                    "Rule matches potentially found but problems encountered parsing them, so no match data to upload."
                )
                logger.info("Please contact support.\n")
        else:
            logger.info(
                "Found %d rule match%s.",
                self.matches,
                "es" if self.matches > 1 else "",
            )
            if not config.test_scan:
                logger.info(
                    "Please visit %s for more information\n", config.malware_app_url
                )

        # This is what is uploaded to the malware backend
        return (self.host_scan_mutation, self.host_scan)
